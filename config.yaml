model_name: unsloth/Meta-Llama-3.1-8B-bnb-4bit
max_seq_len: 1024
target_vram_gib: 10
lora:
  r: 16
  alpha: 16
  dropout: 0.0
  target_modules: [q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj]
  use_rslora: true
  gradient_checkpointing: unsloth
train:
  train_file: data/acronyms.train.jsonl
  eval_file: data/acronyms.eval.jsonl
  num_train_epochs: 20
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 8
  learning_rate: 1.0e-4
  lr_scheduler_type: cosine
  warmup_ratio: 0.03
  weight_decay: 0.0
  logging_steps: 10
  eval_steps: 200
  save_steps: 200
  bf16: true
  seed: 3407
  output_dir: outputs/llama31-8b-acronyms
